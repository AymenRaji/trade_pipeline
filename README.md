# ğŸ¦™ Alpaca Trade Data Pipeline

This project implements an end-to-end data pipeline that retrieves trading data and market news from the [Alpaca API](https://alpaca.markets/), loads it into **Snowflake** using [dlt](https://github.com/dlt-hub/dlt), orchestrates the process with **Dagster**, and transforms the raw data into analytical models using **dbt**.

---

## Tech Stack

- **[dlt](https://docs.dlt.dev/)** â€“ Python-based ELT framework for loading data into Snowflake
- **[Dagster](https://dagster.io/)** â€“ Modern orchestrator for data assets and pipelines
- **[dbt](https://docs.getdbt.com/)** â€“ SQL-based transformation framework
- **Docker + Docker Compose** â€“ Containerized development environment
- **Snowflake** â€“ Cloud data warehouse (destination)

---

## ğŸ“ Project Structure

```bash
trade_pipeline/
â”‚
â”œâ”€â”€ .dlt/   # dlt secrets & source config
â”‚   â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ config.toml
â”‚   â””â”€â”€ secrets.toml
â”‚
â”œâ”€â”€ dagster/          
â”‚   â””â”€â”€ alpaca_pipeline/
â”‚       â”œâ”€â”€ assets/
â”‚       â”‚   â”œâ”€â”€ alpaca_dbt_assets.py
â”‚       â”‚   â””â”€â”€ alpaca_dlt_assets.py
â”‚       â”œâ”€â”€ jobs.py
â”‚       â”œâ”€â”€ schedules.py
â”‚       â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ dagster.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ workspace.yaml
â”‚
â”œâ”€â”€ dbt_alpaca/           
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ target/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles_example/
â”‚
â”œâ”€â”€ dlt_pipeline/
â”‚   â”œâ”€â”€ helpers.py             
â”‚   â”œâ”€â”€ resource.py             
â”‚   â”œâ”€â”€ settings.py           
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ image.png   # Dagster lineage screenshot
â””â”€â”€ README.md


```
---

##  Docker Compose Setup

This project is implemented using a `docker-compose.yml` file that sets up a multi-container environment to run the ETL pipeline and supporting services. The stack consists of the following components:

- **PostgreSQL**: Serves as the metadata and configuration store for dagster pipelines.
- **dlt**: Handles building DLT pipeline image and execution.
- **Dagster**: Provides the orchestration and execution engine for managing ELT workflows.
- **dagster_webserver**: The web-based UI for Dagster, used to monitor and trigger pipelines.

All services are configured to run in an isolated Docker network and communicate internally. Environment variables are used to configure credentials and settings for secure and flexible deployment.



###  Getting Started

To launch the project, run the following command from the project root directory:

```bash
docker-compose up --build

```
To stop the project, run the following command from the project root directory:
```bash 
  docker-compose down
```
To retard the project, run the following command stopCMD; launch CMD from the project root directory:

```bash
  docker-compose down; docker-compose up --build
```

---


# ğŸ“Š DLT Pipeline for Alpaca 

This`dlt_pipeline` that retrieves **daily stock statistics** and **daily share news** from the **Alpaca API** using **OAuth2 authentication**.


1. The pipeline starts by sending an initial **GET request** to the Alpaca API to fetch both `daily_stock_statistics` and `daily_share_news`.

2. It then uses **pagination** to collect the complete set of results.

3. Once all pages are retrieved, the data is **flattened** to a structured format.

4. The processed data is **yielded to a DLT resource** using the `@dlt.resource` decorator.

5. Both resources (stock statistics and news) are combined into a single **DLT source** using the `@dlt.source` decorator.

6. This unified source is then passed to **Dagster**, which orchestrates and runs the pipeline.

## âš™ï¸ Features

- Retrieves data from the Alpaca API using OAuth2
- Handles pagination to ensure complete data retrieval
- Flattens nested JSON responses for consistency
- Combines multiple data resources into a single source
- Seamlessly integrates with Dagster for orchestration



## ğŸ“ Required Configuration: `secrets.toml`

To run the pipeline, you must create a `secrets.toml` file with the required credentials for both the data **destination** (e.g., Snowflake) and the **source** (Alpaca API).

```toml
# Destination configuration for Snowflake
[destination.snowflake.credentials]
database = "******"         # fill this in!
password = "********"       # fill this in!
username = "*****"          # fill this in!
host = "*********"          # fill this in!
warehouse = "******"        # fill this in!
role = "*******"            # fill this in!

# Source configuration for Alpaca API
[sources.credentials]
API_KEY = "alpaca_key"              # fill this in!
SECRET_KEY = "alpaca_secret_key"    # fill this in!
```
---





![alt text](image.png)
